# Requirements Document: Phase 2 - RAG Core

## Introduction

This document defines the requirements for Phase 2 (RAG Core) of the Iubar project. Phase 2 builds on the foundation established in Phase 1 to implement the core RAG (Retrieval-Augmented Generation) functionality, enabling users to have contextual AI conversations about their uploaded documents.

Phase 2 focuses on:
1. Semantic search and context retrieval from vector store
2. DeepSeek V3.2-Exp LLM integration with streaming responses
3. Response caching for cost optimization
4. Split-pane chat interface with document viewer
5. Letter-level focus caret for contextual awareness
6. Source attribution linking responses to document sections

## Glossary

- **RAG_Service**: The backend service (`backend/app/services/rag_service.py`) responsible for orchestrating retrieval and generation
- **Query_Embedding**: A 512-dimensional vector representation of a user's question, generated by Voyage AI
- **Context_Window**: The collection of relevant document chunks retrieved for a query, typically top 5-10 by similarity
- **Response_Cache**: An in-memory cache mapping query hashes to previously generated responses
- **Chat_Session**: A conversation thread with message history, stored in SQLite
- **Focus_Caret**: A visual indicator (spark/glow) highlighting a specific letter in the document viewer
- **Source_Attribution**: Metadata linking AI response segments to specific document chunks
- **Streaming_Response**: Server-Sent Events (SSE) delivering AI response tokens incrementally
- **Cosine_Similarity**: Distance metric for vector comparison (lower = more similar, range 0-2)
- **System_Prompt**: Instructions defining AI personality and behavior (Adaptive Socratic style)
- **Token_Budget**: Maximum tokens allowed in context window (DeepSeek: 128K, target: 8K for cost)

## Requirements

### Requirement 1: Chat Session Management

**User Story:** As a user, I want to start a chat session about a document, so that I can ask questions and receive contextual answers.

#### Acceptance Criteria

1. THE Backend SHALL expose `POST /api/chat/sessions` endpoint accepting JSON body:
   ```json
   {
     "document_id": "uuid-string"  // optional, null for general chat
   }
   ```
2. WHEN a session is created, THE Backend SHALL return JSON response within 200ms:
   ```json
   {
     "session_id": "uuid-string",
     "document_id": "uuid-string",  // or null
     "created_at": "2026-01-17T10:30:00Z",
     "message_count": 0
   }
   ```
3. THE Backend SHALL create a `chat_sessions` table with columns:
   - `id` (TEXT PRIMARY KEY)
   - `document_id` (TEXT, FOREIGN KEY to documents.id, nullable)
   - `created_at` (TEXT, ISO 8601 timestamp)
   - `updated_at` (TEXT, ISO 8601 timestamp)
   - `metadata` (TEXT, JSON string for future extensions)
4. THE Backend SHALL expose `GET /api/chat/sessions` returning list of all sessions
5. THE Backend SHALL expose `GET /api/chat/sessions/{id}` returning session details with message history
6. THE Backend SHALL expose `DELETE /api/chat/sessions/{id}` which cascades to delete all messages
7. IF a session is created with a document_id that doesn't exist, THE Backend SHALL return HTTP 404 with error: `"Document not found"`

### Requirement 2: Message Storage

**User Story:** As a user, I want my chat history preserved, so that I can review previous conversations.

#### Acceptance Criteria

1. THE Backend SHALL create a `chat_messages` table with columns:
   - `id` (TEXT PRIMARY KEY)
   - `session_id` (TEXT, FOREIGN KEY to chat_sessions.id ON DELETE CASCADE)
   - `role` (TEXT, CHECK IN ('user', 'assistant'))
   - `content` (TEXT, NOT NULL)
   - `created_at` (TEXT, ISO 8601 timestamp)
   - `metadata` (TEXT, JSON string containing source_chunks, focus_context, token_count)
2. WHEN a user sends a message, THE Backend SHALL insert a record with role='user'
3. WHEN the AI responds, THE Backend SHALL insert a record with role='assistant'
4. THE Backend SHALL store source attribution in metadata as:
   ```json
   {
     "source_chunks": [
       {"chunk_id": "uuid", "document_id": "uuid", "similarity": 0.15}
     ],
     "focus_context": {"start_char": 100, "end_char": 200},
     "token_count": 450
   }
   ```
5. THE Backend SHALL expose `GET /api/chat/sessions/{id}/messages` returning messages ordered by created_at ASC
6. THE Backend SHALL limit message history to most recent 50 messages per session for performance

### Requirement 3: Semantic Search and Context Retrieval

**User Story:** As a system, I need to find relevant document chunks for a query, so that the AI has appropriate context.

#### Acceptance Criteria

1. THE RAG_Service SHALL implement `retrieve_context()` method accepting:
   - `query: str` - user's question
   - `document_id: Optional[str]` - limit search to specific document
   - `n_results: int` - number of chunks to retrieve (default 5)
   - `similarity_threshold: float` - minimum cosine similarity (default 0.7, range 0.7-0.9 for strict matching)
2. WHEN `retrieve_context()` is called WITHOUT a document_id, THE RAG_Service SHALL:
   - Generate query embedding via Embedding_Service.embed_query()
   - Retrieve document summaries from all documents
   - Compare query embedding against document summary embeddings
   - Select top 5 most relevant documents based on summary similarity
   - Search for chunks within those selected documents only
   - Filter results by similarity_threshold
   - Return top n_results chunks across selected documents
3. WHEN `retrieve_context()` is called WITH a document_id, THE RAG_Service SHALL:
   - Generate query embedding via Embedding_Service.embed_query()
   - Query Vector_Store with the embedding, filtered by document_id
   - Filter results by similarity_threshold
   - Return top n_results chunks
4. THE RAG_Service SHALL return a `RetrievalResult` dataclass containing:
   ```python
   @dataclass
   class RetrievalResult:
       chunks: List[RetrievedChunk]
       total_tokens: int
       query_embedding_time_ms: float
       search_time_ms: float
       selected_documents: List[str]  # document_ids used in search
   
   @dataclass
   class RetrievedChunk:
       chunk_id: str
       document_id: str
       content: str
       similarity: float  # cosine similarity (higher = more similar, range 0-1)
       metadata: dict  # chunk_index, start_char, end_char
   ```
5. IF no chunks meet the similarity_threshold, THE RAG_Service SHALL return empty chunks list
6. THE RAG_Service SHALL log retrieval metrics (embedding time, search time, chunk count, selected documents) at INFO level asynchronously without blocking
7. THE RAG_Service SHALL enforce a maximum context window of 8000 tokens (sum of chunk tokens)
8. IF retrieved chunks exceed 8000 tokens, THE RAG_Service SHALL truncate to top N chunks that fit
9. THE Backend SHALL create a `document_summaries` table with columns:
   - `document_id` (TEXT PRIMARY KEY, FOREIGN KEY to documents.id ON DELETE CASCADE)
   - `summary_text` (TEXT, NOT NULL, max 500 characters)
   - `summary_embedding` (BLOB, 512-dimensional vector as bytes)
   - `created_at` (TEXT, ISO 8601 timestamp)
10. THE Backend SHALL generate document summaries asynchronously after document processing completes
11. THE Document summary SHALL be generated using the first 2000 characters of the document with a prompt:
    ```
    Summarize this document in 2-3 sentences focusing on the main topics and themes:
    
    {document_preview}
    ```

**Design Note**: Cosine similarity threshold of 0.7 is recommended based on RAG best practices. Values between 0.7-0.9 provide strict matching, ensuring high-quality retrieval. Lower thresholds (0.5-0.7) can be used for broader recall if needed, configurable via environment variable `RAG_SIMILARITY_THRESHOLD`.

### Requirement 4: Focus Caret Context Integration

**User Story:** As a user, I want the AI to understand which part of the document I'm focused on, so that responses are contextually relevant.

#### Acceptance Criteria

1. THE Frontend SHALL send focus caret position with queries as:
   ```json
   {
     "message": "Explain this section",
     "focus_context": {
       "document_id": "uuid",
       "start_char": 1500,
       "end_char": 1800,
       "surrounding_text": "...context around focused letter..."
     }
   }
   ```
2. WHEN focus_context is provided, THE RAG_Service SHALL:
   - Identify the chunk containing the focus position (start_char to end_char range)
   - Boost that chunk's similarity score by 0.15 (making it more likely to be included)
   - Include the surrounding_text in the system prompt as "User is focused on: {text}"
3. THE RAG_Service SHALL store focus_context in message metadata for reference
4. IF focus_context.document_id doesn't match the session's document_id, THE RAG_Service SHALL log a warning asynchronously but proceed

**Design Note**: Removed forced inclusion of focused chunk (req 4.3 from original). The similarity boost of 0.15 is sufficient to prioritize the focused chunk without forcing potentially irrelevant content into the context window. This maintains retrieval quality while respecting user focus.

### Requirement 5: LLM Integration (DeepSeek V3.2-Exp)

**User Story:** As a system, I need to generate AI responses using retrieved context, so that users receive accurate, contextual answers.

#### Acceptance Criteria

1. THE RAG_Service SHALL integrate DeepSeek V3.2-Exp via OpenAI-compatible API:
   - Base URL: `https://api.deepseek.com/v1`
   - Model: `deepseek-chat`
   - API Key: from `DEEPSEEK_API_KEY` environment variable
2. THE RAG_Service SHALL construct prompts with the following structure:
   ```
   System: {adaptive_socratic_prompt}
   
   Context from documents:
   [Document: {doc_title}]
   {chunk_1_content}
   
   [Document: {doc_title}]
   {chunk_2_content}
   
   User is focused on: {focus_context.surrounding_text}
   
   User: {user_message}
   ```
3. THE RAG_Service SHALL use the following system prompt (sent only at session start):
   ```
   You are an AI learning instructor with knowledge of all scientific facts and proven techniques that help with learning.
   Guide through questions rather than just providing answers - using the Socratic method. Be direct and honest.
   
   Rules:
   - Sparse praise: Only acknowledge genuine insights or real effort
   - No empty validation: Avoid "Great question!" patterns
   - Challenge assumptions gently: "What makes you think that?"
   - Guide discovery: "What do you notice about this pattern?"
   - Reference context: Always cite which document section you're using
   
   When answering:
   1. Assess user's level from their question or via user profile
   2. Ask clarifying questions when helpful
   3. Provide direct answers when clearly needed or explicitly requested
   4. Connect to previous conversation context
   5. Cite sources: [Source: Document Title, Section]
   ```
4. THE RAG_Service SHALL call DeepSeek API with the following parameters:
   - `model` (string, required): `"deepseek-chat"` - The model identifier
   - `messages` (array, required): Constructed prompt array with role/content objects
   - `temperature` (float, optional, 0-2, default 0.7): Controls randomness. Higher = more creative, lower = more focused
   - `max_tokens` (integer, optional, default 2000): Maximum tokens in response. Does not summarize, just stops at limit
   - `top_p` (float, optional, 0-1, default 1.0): Nucleus sampling. Alternative to temperature. Use one or the other
   - `frequency_penalty` (float, optional, -2 to 2, default 0): Penalizes repeated tokens. Positive = less repetition
   - `presence_penalty` (float, optional, -2 to 2, default 0): Penalizes tokens based on presence. Positive = more novel topics
   - `stream` (boolean, optional, default false): Enable streaming responses via SSE
   - `stop` (string or array, optional): Sequences where API stops generating
   - `logprobs` (boolean, optional): Return log probabilities of output tokens
   - `top_logprobs` (integer, optional, 0-20): Number of most likely tokens to return at each position
5. THE RAG_Service SHALL use these parameter values for Iubar:
   - `temperature`: `0.6` (balanced creativity for educational responses)
   - `max_tokens`: `2000` (sufficient for detailed explanations)
   - `frequency_penalty`: `0.3` (slight reduction in repetition)
   - `presence_penalty`: `0.1` (slight encouragement for topic diversity)
   - `stream`: `true` (enable streaming)
6. THE RAG_Service SHALL track token usage from API response:
   - `prompt_tokens`: input tokens (context + query)
   - `completion_tokens`: output tokens (response)
   - `total_tokens`: sum of both
7. THE RAG_Service SHALL calculate estimated cost:
   - Input: `prompt_tokens * $0.28 / 1_000_000`
   - Output: `completion_tokens * $0.42 / 1_000_000`
   - Cached input: `cached_tokens * $0.028 / 1_000_000` (90% cheaper)
8. IF DeepSeek API returns HTTP 401, THE RAG_Service SHALL raise error: `"Configuration error. Please contact support."`
9. IF DeepSeek API returns HTTP 429, THE RAG_Service SHALL wait 60 seconds and retry up to 3 times
10. IF DeepSeek API returns HTTP 5xx, THE RAG_Service SHALL retry with exponential backoff (5s, 10s, 20s)

**Design Note**: DeepSeek API is OpenAI-compatible. Parameters like `temperature`, `top_p`, `frequency_penalty`, and `presence_penalty` follow OpenAI conventions. The `deepseek-reasoner` model (thinking mode) does not support these parameters and will ignore them without error.

### Requirement 6: Streaming Response Delivery

**User Story:** As a user, I want to see AI responses appear progressively, so that I get immediate feedback and can read while the AI is still generating.

#### Acceptance Criteria

1. THE Backend SHALL expose `POST /api/chat/sessions/{id}/messages` endpoint accepting:
   ```json
   {
     "message": "user question",
     "focus_context": {  // optional
       "document_id": "uuid",
       "start_char": 1500,
       "end_char": 1800,
       "surrounding_text": "..."
     }
   }
   ```
2. THE Backend SHALL return streaming response using Server-Sent Events (SSE) with Content-Type: `text/event-stream`
3. THE Backend SHALL stream events in the following format:
   ```
   event: token
   data: {"content": "word "}
   
   event: token
   data: {"content": "by "}
   
   event: source
   data: {"chunk_id": "uuid", "document_id": "uuid", "similarity": 0.15}
   
   event: done
   data: {"message_id": "uuid", "token_count": 450, "cost_usd": 0.000126}
   ```
4. THE Backend SHALL send `event: token` for each token received from DeepSeek streaming API
5. THE Backend SHALL send `event: source` once after all tokens, containing source attribution
6. THE Backend SHALL send `event: done` as the final event with metadata
7. THE Backend SHALL save the complete message to database only after streaming completes
8. IF streaming is interrupted (client disconnect), THE Backend SHALL:
   - Stop the DeepSeek API stream
   - Save partial response to database with metadata: `{"interrupted": true}`
   - Log the interruption at WARNING level
9. THE Backend SHALL implement connection timeout of 60 seconds for streaming responses
10. THE Backend SHALL handle DeepSeek API errors during streaming by sending:
    ```
    event: error
    data: {"error": "Service temporarily unavailable. Please try again."}
    ```

### Requirement 7: Response Caching

**User Story:** As a system, I need to cache responses for similar queries, so that costs are minimized and responses are faster.

#### Acceptance Criteria

1. THE RAG_Service SHALL implement an in-memory response cache using LRU (Least Recently Used) eviction
2. THE Cache SHALL have a maximum size of 500 entries (configurable via `RESPONSE_CACHE_SIZE` env var)
3. THE Cache key SHALL be computed as:
   ```python
   cache_key = hashlib.sha256(
       f"{query_text}:{document_id}:{focus_context_hash}".encode()
   ).hexdigest()
   ```
4. THE Cache value SHALL contain:
   ```python
   @dataclass
   class CachedResponse:
       response_text: str
       source_chunks: List[dict]
       token_count: int
       created_at: datetime
       hit_count: int  # number of times this cache entry was used
   ```
5. WHEN a query is received, THE RAG_Service SHALL:
   - Compute cache key
   - Check if key exists in cache
   - If hit: return cached response, increment hit_count, skip LLM call
   - If miss: proceed with retrieval and generation, store result in cache
6. THE RAG_Service SHALL log cache hits at INFO level asynchronously without blocking: `"Cache hit for query: {query[:50]}..."`
7. THE RAG_Service SHALL log cache statistics every 50 queries asynchronously:
   - Total queries
   - Cache hits
   - Cache misses
   - Hit rate percentage
   - Estimated cost savings
8. THE Cache SHALL expire entries after 24 hours (configurable via `CACHE_TTL_HOURS` env var)
9. THE Cache SHALL support manual invalidation via `POST /api/cache/clear` endpoint (admin only)

**Design Note**: Reduced cache size from 1000 to 500 entries for faster lookup performance. With 100 entries, cache lookup remains O(1) with minimal memory overhead (~10-20MB). LRU eviction ensures most frequently accessed responses stay cached. Larger caches can be enabled via environment variable if needed.

### Requirement 8: Source Attribution

**User Story:** As a user, I want to see which document sections the AI used for its response, so that I can verify information and explore further.

#### Acceptance Criteria

1. THE RAG_Service SHALL track which chunks were used in the context window
2. THE RAG_Service SHALL include source attribution in the streaming response as `event: source`
3. THE Source attribution SHALL contain:
   ```json
   {
     "chunk_id": "uuid",
     "document_id": "uuid",
     "document_title": "Document Title",
     "chunk_index": 5,
     "similarity": 0.85,
     "start_char": 1500,
     "end_char": 1800
   }
   ```
4. THE Frontend SHALL display source attribution as clickable links below the AI response
5. WHEN a user clicks a source link, THE Frontend SHALL:
   - Scroll the document viewer to the referenced chunk
   - Highlight the chunk with a subtle background color (#253550)
   - Place the focus caret at the start of the chunk
6. THE Frontend SHALL display source attribution with each section as an individual link:
   ```
   Sources:
   • Document Title - Section 5
   • Document Title - Section 7
   ```
7. IF multiple chunks are from the same document, THE Frontend SHALL group them: `Document Title (Sections 5, 7, 9)` with each section number a clickable link which takes the user to that section

### Requirement 9: Split-Pane Chat Interface

**User Story:** As a user, I want to see my document and chat side-by-side, so that I can reference the content while asking questions.

#### Acceptance Criteria

1. THE Frontend SHALL implement a split-pane layout with:
   - Left pane (70% width): Document viewer with Markdown rendering
   - Right pane (30% width): Chat interface
   - Resizable border between panes (drag to adjust)
2. THE Document viewer SHALL:
   - Render Markdown with syntax highlighting for code blocks
   - Support scrolling independently from chat pane
   - Display focus caret (letter-level glow) at user-selected position
   - Highlight referenced chunks when source links are clicked
3. THE Chat interface SHALL:
   - Display message history (user and assistant messages)
   - Show streaming responses with a light-based thinking indicator (pulsing glow effect aligned with visual identity)
   - Display source attribution links below assistant messages
   - Provide input field at bottom with send button
   - Auto-scroll to latest message
4. THE Resizable border SHALL:
   - Show cursor change to `col-resize` on hover
   - Allow dragging to adjust pane widths
   - Enforce minimum widths: 40% for document, 20% for chat
   - Persist user's preferred width to localStorage
5. THE Frontend SHALL display a loading state while document is being fetched
6. THE Document viewer pane SHALL be collapsible:
   - When no document is selected, show collapsed state with expand button
   - Expand button SHALL display icon/indicator aligned with light-based visual identity
   - When collapsed, chat pane takes full width
   - Expand button remains visible on left edge for easy access
   - User can toggle collapse/expand at any time
7. THE Frontend SHALL display an empty state message when document viewer is collapsed:
   ```
   "Select a document to start chatting, or ask a general question"
   ```

**Design Note**: The thinking indicator uses a pulsing glow effect (similar to focus caret) rather than traditional typing dots, maintaining consistency with the "light through clarity" visual identity. The glow pulses between opacity 0.5 and 1.0 with a 1.5s cycle using the golden accent color (#D4A574).

### Requirement 10: Focus Caret Implementation

**User Story:** As a user, I want to click on a word in the document to focus the AI's attention on that section, so that my questions are contextually relevant.

#### Acceptance Criteria

1. THE Frontend SHALL implement a letter-level focus caret as a glowing indicator
2. THE Focus caret SHALL be positioned on a single letter within a word (anchor letter, typically at 40% of word length)
3. THE Focus caret visual design SHALL be:
   - Glow color: `rgba(212, 165, 116, 0.5)` (golden with transparency)
   - Blur radius: `2px`
   - Spread: `1px` (tight halo)
   - Animation: Fade in over 200ms with cubic-bezier(0.4, 0, 0.2, 1) easing
4. WHEN a user clicks on a word, THE Frontend SHALL:
   - Calculate the anchor letter position (40% of word length)
   - Apply the focus caret glow to that letter
   - Extract surrounding context (±150 characters)
   - Store focus_context in state for next query
5. THE Focus caret SHALL support keyboard navigation:
   - Arrow Up: Move caret to previous paragraph
   - Arrow Down: Move caret to next paragraph
   - Arrow left: Move caret to previous character
   - Arrow right: Move caret to next character
6. THE Frontend SHALL send focus_context with queries:
   ```json
   {
     "document_id": "uuid",
     "start_char": 1500,
     "end_char": 1800,
     "surrounding_text": "...context around focused letter..."
   }
   ```
7. THE Focus caret SHALL remain visible until:
   - User clicks elsewhere in document, after which it reappers where user clicked
   - User scrolls more than 2 screen heights away
8. THE Frontend SHALL display a subtle hint on first document load:
   ```
   "Click any word to focus AI's attention on that section"
   ```
   (Dismissible, shown only once per session)

### Requirement 11: Cost Tracking Display

**User Story:** As a user, I want to see how much my queries cost, so that I'm aware of usage and can optimize if needed.

#### Acceptance Criteria

1. THE Backend SHALL track cumulative costs per session in `chat_sessions.metadata`:
   ```json
   {
     "total_tokens": 15000,
     "prompt_tokens": 10000,
     "completion_tokens": 5000,
     "cached_tokens": 3000,
     "estimated_cost_usd": 0.00456
   }
   ```
2. THE Backend SHALL update session metadata after each message asynchronously without blocking the response flow
3. THE Backend SHALL expose `GET /api/chat/sessions/{id}/stats` returning:
   ```json
   {
     "message_count": 12,
     "total_tokens": 15000,
     "estimated_cost_usd": 0.00456,
     "cache_hit_rate": 0.25,
     "avg_response_time_ms": 1850
   }
   ```
4. THE Frontend SHALL display cost information in the chat interface header:
   ```
   "12 messages • $0.0046 • 25% cached"
   ```
5. THE Frontend SHALL update cost display after each message
6. THE Frontend SHALL display a tooltip on hover explaining cost breakdown:
   ```
   "Input: $0.0028 (10K tokens)
    Output: $0.0021 (5K tokens)
    Cached: $0.0008 (3K tokens, 90% savings)"
   ```

### Requirement 12: Suggested Questions

**User Story:** As a user, I want to see suggested questions after uploading a document, so that I know what kinds of questions I can ask.

#### Acceptance Criteria

1. THE Backend SHALL generate suggested questions after document processing completes
2. THE Backend SHALL use the document summary (from document_summaries table) with a lightweight prompt to DeepSeek:
   ```
   Based on this document summary, suggest 3 thought-provoking questions a learner might wish to know:
   
   Title: {document_title}
   Summary: {document_summary}
   
   Format: Return only the questions, one per line, no numbering.
   ```
3. THE Backend SHALL store suggested questions in `documents.metadata`:
   ```json
   {
     "title": "Document Title",
     "suggested_questions": [
       "What is the main argument of this paper?",
       "How does this concept relate to X?",
       "Can you explain the methodology used?"
     ]
   }
   ```
4. THE Backend SHALL limit suggested questions to 3 per document
5. THE Frontend SHALL display suggested questions below the document title:
   ```
   Suggested questions:
   • What is the main argument of this paper?
   • How does this concept relate to X?
   • Can you explain the methodology used?
   ```
6. WHEN a user clicks a suggested question, THE Frontend SHALL:
   - Populate the chat input with the question
   - Focus the input field
   - NOT automatically send (user must click send)
7. THE Backend SHALL generate suggested questions asynchronously (not blocking document processing)
8. IF suggested question generation fails, THE Backend SHALL log error but NOT fail document processing

### Requirement 13: Error Handling and User Feedback

**User Story:** As a user, I want clear error messages when something goes wrong, so that I understand what happened and how to fix it.

#### Acceptance Criteria

1. THE Backend SHALL map RAG-specific errors to user-friendly messages:
   | Internal Error | User Message |
   |----------------|--------------|
   | `EmbeddingError` | `"Could not process your question. Please try again."` |
   | `VectorStoreError` | `"Search temporarily unavailable. Please try again."` |
   | `DeepSeekAPIError` (401) | `"Configuration error. Please contact support."` |
   | `DeepSeekAPIError` (429) | `"Service is busy. Please try again in a minute."` |
   | `DeepSeekAPIError` (5xx) | `"AI service temporarily unavailable. Please try again."` |
   | `SessionNotFoundError` | `"Chat session not found. It may have been deleted."` |
   | `DocumentNotFoundError` | `"Document not found. It may have been deleted."` |
2. THE Frontend SHALL display error messages in the chat interface as system messages:
   ```
   [System] Could not process your question. Please try again.
   ```
3. THE Frontend SHALL provide a retry button for failed messages
4. THE Frontend SHALL display connection status indicator:
   - Green dot: Connected
   - Yellow dot: Connecting...
   - Red dot: Disconnected (with reconnect button)
5. THE Frontend SHALL handle streaming interruptions gracefully:
   - Display partial response with indicator: `[Response interrupted]`
   - Provide retry button
6. THE Backend SHALL log all errors with full context (session_id, query, error details) at ERROR level
7. THE Backend SHALL NOT expose internal error details (stack traces, file paths) to users

### Requirement 14: Performance and Optimization

**User Story:** As a user, I want fast responses, so that the conversation feels natural and engaging.

#### Acceptance Criteria

1. THE RAG_Service SHALL target the following performance metrics:
   - Query embedding: < 200ms
   - Vector search: < 100ms
   - LLM first token: < 1000ms
   - LLM streaming: 20-30 tokens/second
   - Total time to first token: < 2000ms
2. THE RAG_Service SHALL implement connection pooling for:
   - Voyage AI API (library default unless more is necessary)
   - DeepSeek API (library default unless more is necessary)
   - SQLite database (library default unless more is necessary)
3. THE RAG_Service SHALL use async/await for all I/O operations
4. THE RAG_Service SHALL batch database writes where possible:
   - Insert user message and assistant message in single transaction
   - Update session metadata in same transaction
5. THE Frontend SHALL implement optimistic UI updates:
   - Show user message immediately (before server confirmation)
   - Show typing indicator immediately (before first token)
   - Update message count and cost immediately (before server response)
6. THE Frontend SHALL debounce focus caret updates (200ms) to avoid excessive re-renders
7. THE Backend SHALL implement request timeout of 30 seconds for non-streaming endpoints
8. THE Backend SHALL implement streaming timeout of 60 seconds for SSE connections
9. THE Backend SHALL log performance metrics for each query:
   ```
   INFO: Query processed in 1850ms (embed: 180ms, search: 95ms, llm: 1575ms)
   ```

### Requirement 15: API Documentation

**User Story:** As a developer, I want comprehensive API documentation, so that I can integrate with the backend or debug issues.

#### Acceptance Criteria

1. THE Backend SHALL expose OpenAPI/Swagger documentation at `/docs`
2. THE Documentation SHALL include all RAG endpoints with:
   - Request/response schemas
   - Example requests and responses
   - Error codes and messages
   - Authentication requirements (if any)
3. THE Documentation SHALL include streaming endpoint details:
   - SSE event types
   - Event data schemas
   - Connection handling
4. THE Backend SHALL include docstrings for all public methods in RAG_Service
5. THE Backend SHALL include type hints for all function parameters and return values

